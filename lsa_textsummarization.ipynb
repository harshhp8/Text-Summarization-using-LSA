{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H:/College WOrk/papers/stop-words.txt',encoding='utf-16') as fp:\n",
    "    v = fp.read()\n",
    "\n",
    "def removestopwords(line):\n",
    "    #print(\"original line\",line)\n",
    "    querywords = line.split()\n",
    "\n",
    "    resultwords  = [word for word in querywords if word.lower() not in v]\n",
    "    result = ' '.join(resultwords)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def Average(lst):\n",
    "    return mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge.rouge import rouge_n_sentence_level\n",
    "from rouge import rouge_n_summary_level\n",
    "from rouge import rouge_l_summary_level\n",
    "from rouge import rouge_w_sentence_level\n",
    "from rouge import rouge_w_summary_level\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "def comparerouge1(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_1 = rouge_n_summary_level(tokenize_words, tokenize_words1, 1)\n",
    "    #rouge1.append(rouge_1)\n",
    "    #print('ROUGE-1: %f' % rouge_1)\n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "    #print('ROUGE-2: %f' % rouge_2)\n",
    "    #rouge2.append(rouge_2)\n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    #print('ROUGE-L: %f' % rouge_l)\n",
    "    #rougel.append(rouge_l)\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    #avg=(rouge_1+rouge_2+rouge_l+rouge_w)/4\n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerouge2(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "   \n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "   \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougel(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    \n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougew(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "finallist=[]\n",
    "rouge1=[]\n",
    "rouge2=[]\n",
    "rougel=[]\n",
    "rougew=[]\n",
    "class ShortText:\n",
    "    def __init__(self, my_id, human_label, summary, short_text):\n",
    "        self.id = my_id         \n",
    "        self.human_label = human_label    \n",
    "        self.summary = summary \n",
    "        self.short_text = short_text\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        For printing purposes.\n",
    "        '''\n",
    "        return '%d\\t%d\\t%s\\t%s' % (self.id, self.human_label, self.summary, self.short_text)\n",
    "\n",
    "def load_file(filename,createfile):\n",
    "    tokenize_words=[]\n",
    "    maintext=[]\n",
    "    listofimpcandidate=[]\n",
    "    listwithfinalsentence=[]\n",
    "    file1 = open(createfile, \"w\", encoding=\"utf-8\")\n",
    "    #retrieve the original text \n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    docno=soup.find_all('docno')\n",
    "    titleno = soup.find_all('title')\n",
    "    text=soup.find_all('text')\n",
    "    instances = {}\n",
    "    my_id = 0\n",
    "    for n,tit,maintitle in zip(docno,text,titleno):\n",
    "        tit=tit.get_text()\n",
    "        tit=str(tit)\n",
    "        sentences=sentence_tokenize.sentence_split(tit, lang='gu')\n",
    "        for t in sentences:\n",
    "            sen=removestopwords(t)\n",
    "            #print(\"this is sen\",sen)\n",
    "            maintext.append(sen)\n",
    "        list3 = [''.join(c for c in s if c not in string.punctuation) for s in maintext]\n",
    "        list3 = [s for s in list3 if s]\n",
    "        \n",
    "        bag_of_words = vectorizer.fit_transform(list3)\n",
    "        bag_of_words.todense()\n",
    "        #This process encodes our original data into topic encoded data\n",
    "        svd = TruncatedSVD(n_components = 2)\n",
    "        lsa = svd.fit_transform(bag_of_words)\n",
    "        topic_encoded_df = pd.DataFrame(lsa, columns=[\"topic1\", \"topic2\"])\n",
    "        topic_encoded_df[\"doc\"]= list3\n",
    "        avg1=topic_encoded_df[\"topic1\"].mean()\n",
    "        avg2=topic_encoded_df[\"topic2\"].mean()\n",
    "        topic_encoded_df['topic1'] = np.where(topic_encoded_df['topic1'] < avg1, 0, topic_encoded_df['topic1'])\n",
    "        topic_encoded_df['topic2'] = np.where(topic_encoded_df['topic2'] < avg2, 0, topic_encoded_df['topic2'])\n",
    "        sum_column = topic_encoded_df[\"topic1\"] + topic_encoded_df[\"topic2\"]\n",
    "        topic_encoded_df[\"sum_column\"]=sum_column\n",
    "        sort_by_sumcolumn = topic_encoded_df.sort_values('sum_column',ascending=False)\n",
    "        compressionratio = round((len(sort_by_sumcolumn.index)*50)/100)\n",
    "        #print(compressionratio)\n",
    "        final_df2=sort_by_sumcolumn['doc'].head(compressionratio)\n",
    "        listtoretrieve=list(final_df2.index.values)\n",
    "        listtoretrieve.sort()\n",
    "        originalsen=[]\n",
    "        for t in sentences:    \n",
    "            originalsen.append(t)\n",
    "        df4 = pd.DataFrame(originalsen,columns=['sentences'])\n",
    "        #print(df4)\n",
    "        finaldraft=\"\"\n",
    "        for i in listtoretrieve:\n",
    "            finaldraft+=df4['sentences'].loc[i]\n",
    "        rouge_1=comparerouge1(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        rouge_2=comparerouge2(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        rouge_l=comparerougel(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        rouge_w=comparerougew(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        \n",
    "        rouge1.append(rouge_1)\n",
    "        rouge2.append(rouge_2)\n",
    "        rougel.append(rouge_l)\n",
    "        rougew.append(rouge_w)\n",
    "        \n",
    "        rouge1average=Average(rouge1)\n",
    "        rouge2average=Average(rouge2)\n",
    "        rougelaverage=Average(rougel)\n",
    "        rougewaverage=Average(rougew)\n",
    "        print(\"rouge1 average\",rouge1average)\n",
    "        print(\"rouge2 average\",rouge2average)\n",
    "        print(\"rougel average\",rougelaverage)\n",
    "        print(\"rougew average\",rougewaverage)\n",
    "        #print(finaldraft)            \n",
    "#         originalfull_str = convert_list_to_string(finaldraft)\n",
    "#         originalfull_str=\" \".join(originalfull_str.split())\n",
    "#         n=str(n)\n",
    "        file1.write(finaldraft)\n",
    "        instances = {}    \n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"G:/Movies/gujarati Text summarization dataset/entertainment/\"):    \n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                load_file(os.path.join(root, file),\"G:/Movies/gujarati Text summarization_lda/entertainment/\"+file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
